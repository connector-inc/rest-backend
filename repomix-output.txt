This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: uv.lock
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.dockerignore
.gitignore
.python-version
alembic/env.py
alembic/README
alembic/script.py.mako
app/api/v1/internal/admin.py
app/api/v1/routers/auth.py
app/api/v1/routers/users.py
app/config.py
app/database.py
app/dependencies.py
app/main.py
app/models.py
app/validators.py
compose.yaml
Dockerfile
LICENSE
proxy/Dockerfile
proxy/nginx.conf
pyproject.toml
README.md

================================================================
Files
================================================================

================
File: .dockerignore
================
.venv/
__pycache__/

alembic/
proxy/

LICENSE
*.pyc
uv.lock

================
File: .gitignore
================
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from `dotnet new gitignore`

# dotenv files
.env

# User-specific files
*.rsuser
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Mono auto generated files
mono_crash.*

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
[Ww][Ii][Nn]32/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/
[Ll]ogs/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUnit
*.VisualState.xml
TestResult.xml
nunit-*.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET
project.lock.json
project.fragment.lock.json
artifacts/

# Tye
.tye/

# ASP.NET Scaffolding
ScaffoldingReadMe.txt

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_h.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*_wpftmp.csproj
*.log
*.tlog
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Coverlet is a free, cross platform Code Coverage Tool
coverage*.json
coverage*.xml
coverage*.info

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# NuGet Symbol Packages
*.snupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx
*.appxbundle
*.appxupload

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!?*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser
*- [Bb]ackup.rdl
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio 6 auto-generated project file (contains which files were open etc.)
*.vbp

# Visual Studio 6 workspace and project file (working project files containing files to include in project)
*.dsw
*.dsp

# Visual Studio 6 technical files
*.ncb
*.aps

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# CodeRush personal settings
.cr/personal

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder
.mfractor/

# Local History for Visual Studio
.localhistory/

# Visual Studio History (VSHistory) files
.vshistory/

# BeatPulse healthcheck temp database
healthchecksdb

# Backup folder for Package Reference Convert tool in Visual Studio 2017
MigrationBackup/

# Ionide (cross platform F# VS Code tools) working folder
.ionide/

# Fody - auto-generated XML schema
FodyWeavers.xsd

# VS Code files for those working on multiple tools
.vscode/*
!.vscode/settings.json
!.vscode/tasks.json
!.vscode/launch.json
!.vscode/extensions.json
*.code-workspace

# Local History for Visual Studio Code
.history/

# Windows Installer files from build outputs
*.cab
*.msi
*.msix
*.msm
*.msp

# JetBrains Rider
*.sln.iml
.idea

##
## Visual studio for Mac
##


# globs
Makefile.in
*.userprefs
*.usertasks
config.make
config.status
aclocal.m4
install-sh
autom4te.cache/
*.tar.gz
tarballs/
test-results/

# Mac bundle stuff
*.dmg
*.app

# content below from: https://github.com/github/gitignore/blob/master/Global/macOS.gitignore
# General
.DS_Store
.AppleDouble
.LSOverride

# Icon must end with two \r
Icon


# Thumbnails
._*

# Files that might appear in the root of a volume
.DocumentRevisions-V100
.fseventsd
.Spotlight-V100
.TemporaryItems
.Trashes
.VolumeIcon.icns
.com.apple.timemachine.donotpresent

# Directories potentially created on remote AFP share
.AppleDB
.AppleDesktop
Network Trash Folder
Temporary Items
.apdisk

# content below from: https://github.com/github/gitignore/blob/master/Global/Windows.gitignore
# Windows thumbnail cache files
Thumbs.db
ehthumbs.db
ehthumbs_vista.db

# Dump file
*.stackdump

# Folder config file
[Dd]esktop.ini

# Recycle Bin used on file shares
$RECYCLE.BIN/

# Windows Installer files
*.cab
*.msi
*.msix
*.msm
*.msp

# Windows shortcuts
*.lnk

# Vim temporary swap files
*.swp

.vscode/
certificates/

================
File: .python-version
================
3.11

================
File: alembic/env.py
================
import asyncio
from logging.config import fileConfig

from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config

from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = None

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection: Connection) -> None:
    context.configure(connection=connection, target_metadata=target_metadata)

    with context.begin_transaction():
        context.run_migrations()


async def run_async_migrations() -> None:
    """In this scenario we need to create an Engine
    and associate a connection with the context.

    """

    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""

    asyncio.run(run_async_migrations())


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

================
File: alembic/README
================
Generic single-database configuration with an async dbapi.

================
File: alembic/script.py.mako
================
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}

================
File: app/api/v1/internal/admin.py
================
from fastapi import APIRouter, Depends

from app.dependencies import get_token_header

router = APIRouter(
    prefix="/admin",
    tags=["admin"],
    dependencies=[Depends(get_token_header)],
    responses={418: {"description": "I'm a teapot"}},
)


@router.post("/")
async def update_admin():
    return {"message": "Admin getting schwifty"}

================
File: app/api/v1/routers/auth.py
================
import json
import logging
from datetime import datetime, timedelta, timezone
from typing import Annotated, Optional
from uuid import UUID, uuid4

import jwt
import resend
import resend.exceptions
from fastapi import (
    APIRouter,
    BackgroundTasks,
    HTTPException,
    Request,
    Response,
    status,
)
from fastapi.responses import RedirectResponse
from pydantic import (
    AfterValidator,
    BaseModel,
    EmailStr,
    ValidationError,
)
from sqlmodel import select

from app.config import get_settings
from app.database import SessionDep
from app.database import r as redis
from app.models import User
from app.validators import email_validator

router = APIRouter(prefix="/auth", tags=["auth"])


resend.api_key = get_settings().resend_api_key


def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        expire = datetime.now(timezone.utc) + timedelta(minutes=15)  # Default 15 mins
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(
        to_encode, get_settings().jwt_secret_key, algorithm=get_settings().jwt_algorithm
    )
    return encoded_jwt


def create_refresh_token(subject: str):  # sub is user's email in this case
    expires_delta = timedelta(days=get_settings().session_expiry_days)
    to_encode = {
        "sub": subject,
        "exp": datetime.now(timezone.utc) + expires_delta,
        "jti": str(uuid4()),  # Add a unique identifier (JWT ID)
    }
    encoded_jwt = jwt.encode(
        to_encode, get_settings().jwt_secret_key, algorithm=get_settings().jwt_algorithm
    )
    return encoded_jwt


def decode_jwt(token: str):
    try:
        payload = jwt.decode(
            token,
            get_settings().jwt_secret_key,
            algorithms=[get_settings().jwt_algorithm],
        )
        return payload
    except jwt.ExpiredSignatureError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, detail="Token has expired"
        )
    except jwt.InvalidTokenError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token"
        )


# async def generate_login_token(email: str):
#     payload = {
#         "sub": email,
#         "exp": datetime.now(timezone.utc)
#         + timedelta(minutes=get_settings().verification_email_expiry_minutes),
#     }

#     token = jwt.encode(
#         payload, get_settings().jwt_secret_key, algorithm=get_settings().jwt_algorithm
#     )

#     redis.setex(
#         f"login:{email}", get_settings().verification_email_expiry_minutes * 60, token
#     )

#     return token


# async def verify_token(token: str):
#     try:
#         payload = jwt.decode(
#             token,
#             get_settings().jwt_secret_key,
#             algorithms=[get_settings().jwt_algorithm],
#         )
#         return payload["sub"]
#     except jwt.PyJWTError:
#         raise HTTPException(
#             status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token."
#         )


# async def send_verification_email(to_email: str, token: str):
#     try:
#         login_url = f"{get_settings().web_app_url}/auth/verify?token={token}"
#         html = f"<p>Click the link below to log in:</p><br/><p><a href={login_url}>{login_url}</a></p><br/><p>This link will expire in {get_settings().verification_email_expiry_minutes} minutes.</p>"
#         params: resend.Emails.SendParams = {
#             "from": f"Connector <{get_settings().sender_email}>",
#             "to": [to_email],
#             "subject": "Log in to Connector",
#             "html": html,
#         }
#         resend.Emails.send(params)
#     except resend.exceptions.ResendError:
#         raise HTTPException(
#             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
#             detail="Email sending failed.",
#         )


# class LoginRequest(BaseModel):
#     email: Annotated[EmailStr, AfterValidator(email_validator)]


# @router.post(
#     "/login",
#     responses={
#         status.HTTP_400_BAD_REQUEST: {"description": "Bad Request"},
#         status.HTTP_500_INTERNAL_SERVER_ERROR: {"description": "Internal Server Error"},
#     },
# )
# async def request_login(request: LoginRequest, background_tasks: BackgroundTasks):
#     try:
#         email = request.email
#         token = await generate_login_token(email)
#         background_tasks.add_task(send_verification_email, email, token)
#         return {"message": "Login email sent"}
#     except ValidationError as e:
#         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
#     except ValueError as e:
#         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))


# @router.get(
#     "/verify",
#     status_code=status.HTTP_307_TEMPORARY_REDIRECT,
#     responses={
#         status.HTTP_401_UNAUTHORIZED: {"description": "Unauthorized"},
#     },
# )
# async def verify(token: str):
#     try:
#         email = await verify_token(token)
#         is_valid_token = redis.get(f"login:{email}") == token

#         if not is_valid_token:
#             response = RedirectResponse(
#                 url=f"{get_settings().web_app_url}/login?error=invalid_token"
#             )
#             return response

#         redis.delete(f"login:{email}")

#         session_id = str(uuid4())
#         redis.setex(
#             f"session:{session_id}",
#             get_settings().session_expiry_days * 24 * 60 * 60,
#             json.dumps(
#                 {
#                     "user_email": email,
#                     "created_at": datetime.now(timezone.utc).isoformat(),
#                     "last_active": datetime.now(timezone.utc).isoformat(),
#                 }
#             ),
#         )

#         response = RedirectResponse(url=f"{get_settings().web_app_url}/")
#         response.set_cookie(
#             key="session_id",
#             value=session_id,
#             max_age=365 * 24 * 60 * 60,  # 365 days
#             samesite="strict",
#             httponly=True,
#             secure=True,
#             domain=get_settings().cookie_domain,
#         )
#         return response
#     except Exception as e:
#         logging.error(e)
#         response = RedirectResponse(
#             url=f"{get_settings().web_app_url}/login?error=invalid_token"
#         )
#         return response


# @router.get(
#     "/",
#     status_code=status.HTTP_204_NO_CONTENT,
#     responses={
#         status.HTTP_401_UNAUTHORIZED: {"description": "Unauthorized"},
#         status.HTTP_404_NOT_FOUND: {"description": "Not Found"},
#     },
# )
# async def get_session(
#     db_session: SessionDep,
#     session_id: UUID,
#     background_tasks: BackgroundTasks,
# ):
#     try:
#         session_key = f"session:{str(session_id)}"
#         session_value: UUID = redis.get(session_key)  # type: ignore

#         if not session_value:
#             raise HTTPException(
#                 status_code=status.HTTP_401_UNAUTHORIZED, detail="Unauthorized."
#             )

#         session_json = json.loads(session_value)

#         session_json["last_active"] = datetime.now(timezone.utc).isoformat()

#         pipe = redis.pipeline()
#         pipe.set(session_key, json.dumps(session_json))
#         pipe.expire(session_key, get_settings().session_expiry_days * 24 * 60 * 60)
#         background_tasks.add_task(pipe.execute)

#         email = session_json.get("user_email")
#         user = await db_session.execute(select(User).where(User.email == email))
#         if not user.scalars().first():
#             raise HTTPException(
#                 status_code=status.HTTP_404_NOT_FOUND, detail="User not found."
#             )
#         return Response(status_code=status.HTTP_204_NO_CONTENT)
#     except HTTPException as e:
#         logging.error(e)
#         raise e
#     except Exception as e:
#         logging.error(e)
#         raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=str(e))


# @router.post(
#     "/logout",
#     status_code=status.HTTP_307_TEMPORARY_REDIRECT,
#     responses={
#         status.HTTP_400_BAD_REQUEST: {"description": "Bad Request"},
#         status.HTTP_401_UNAUTHORIZED: {"description": "Unauthorized"},
#     },
# )
# async def logout(request: Request, background_tasks: BackgroundTasks):
#     try:
#         session_id = request.cookies.get("session_id")
#         if not session_id:
#             raise HTTPException(
#                 status_code=status.HTTP_401_UNAUTHORIZED, detail="Unauthorized."
#             )

#         session_key = f"session:{session_id}"
#         background_tasks.add_task(redis.delete, session_key)

#         response = RedirectResponse(url=f"{get_settings().web_app_url}/")
#         response.delete_cookie("session_id", domain=get_settings().cookie_domain)

#         return response
#     except HTTPException as e:
#         logging.error(e)
#         raise e
#     except Exception as e:
#         logging.error(e)
#         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))

================
File: app/api/v1/routers/users.py
================
import logging

from fastapi import (
    APIRouter,
    Depends,
    HTTPException,
    status,
)
from pydantic import BaseModel
from sqlmodel import select

from app.database import SessionDep
from app.dependencies import get_current_user
from app.models import User, UserGender

router = APIRouter(prefix="/users", tags=["users"])


class CreateUserRequest(BaseModel):
    name: str
    username: str
    gender: str


@router.post(
    "/",
    status_code=status.HTTP_201_CREATED,
    responses={
        status.HTTP_400_BAD_REQUEST: {"description": "Bad Request"},
        status.HTTP_401_UNAUTHORIZED: {"description": "Unauthorized"},
        status.HTTP_409_CONFLICT: {"description": "Conflict"},
    },
)
async def create_user(
    db_session: SessionDep,
    request: CreateUserRequest,
    email: str = Depends(get_current_user),
):
    try:
        user = User(email=email)
        user.name = request.name
        user.username = request.username
        user.gender = UserGender[request.gender]

        db_session.add(user)
        await db_session.commit()
        await db_session.refresh(user)
        return {"message": "User created successfully"}
    except ValueError as e:
        logging.error(e)
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except HTTPException as e:
        logging.error(e)
        raise e
    except Exception as e:
        logging.error(e)
        await db_session.rollback()
        if "unique constraint" in str(e).lower():
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT, detail="User already exists."
            )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail="Failed to create user."
        )


class CheckUsernameAvailabilityRequest(BaseModel):
    username: str


@router.post(
    "/attempt/username",
    responses={
        status.HTTP_400_BAD_REQUEST: {"description": "Bad Request"},
        status.HTTP_401_UNAUTHORIZED: {"description": "Unauthorized"},
    },
)
async def check_username_availability(
    db_session: SessionDep,
    request: CheckUsernameAvailabilityRequest,
    _=Depends(get_current_user),
):
    try:
        username_exists = await db_session.execute(
            select(User).where(User.username == request.username)
        )
        if username_exists.scalars().first():
            return {"message": "Username already exists."}
        return {"message": "Username is available."}
    except HTTPException as e:
        logging.error(e)
        raise e
    except Exception as e:
        logging.error(e)
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail="Failed to check username."
        )


# @router.get(
#     "/me",
#     status_code=status.HTTP_204_NO_CONTENT,
#     responses={
#         status.HTTP_400_BAD_REQUEST: {"description": "Bad Request"},
#         status.HTTP_401_UNAUTHORIZED: {"description": "Unauthorized"},
#         status.HTTP_404_NOT_FOUND: {"description": "Not Found"},
#     },
# )
# async def read_me(
#     db_session: SessionDep,
#     email: str = Depends(get_current_user_email),
# ):
#     try:
#         user = await db_session.execute(select(User).where(User.email == email))
#         if user.scalars().first():
#             return Response(status_code=status.HTTP_204_NO_CONTENT)
#         raise HTTPException(
#             status_code=status.HTTP_404_NOT_FOUND, detail="User not found."
#         )
#     except HTTPException as e:
#         logging.error(e)
#         raise e
#     except Exception as e:
#         logging.error(e)
#         raise HTTPException(
#             status_code=status.HTTP_400_BAD_REQUEST, detail="Failed to get user."
#         )


# @router.get(
#     "/",
#     responses={
#         status.HTTP_400_BAD_REQUEST: {"description": "Bad Request"},
#     },
# )
# async def read_users(
#     db_session: SessionDep,
#     offset: int = 0,
#     limit: Annotated[int, Query(le=10)] = 10,
#     _=Depends(get_current_user_email),
# ):
#     try:
#         users = await db_session.execute(select(User).offset(offset).limit(limit))
#         return users.scalars().all()
#     except ValueError as e:
#         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))


# @router.get(
#     "/{user_id}",
#     responses={
#         status.HTTP_400_BAD_REQUEST: {"description": "Bad Request"},
#         status.HTTP_404_NOT_FOUND: {"description": "Not Found"},
#     },
# )
# async def read_user(
#     db_session: SessionDep,
#     user_id: UUID,
#     _=Depends(get_current_user_email),
# ):
#     try:
#         user = await db_session.get(User, user_id)
#         if not user:
#             raise HTTPException(
#                 status_code=status.HTTP_404_NOT_FOUND, detail="User not found"
#             )
#         return user
#     except ValueError as e:
#         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))


# @router.put(
#     "/{user_id}",
#     responses={
#         status.HTTP_400_BAD_REQUEST: {"description": "Bad Request"},
#         status.HTTP_404_NOT_FOUND: {"description": "Not Found"},
#     },
# )
# async def update_user(
#     db_session: SessionDep,
#     user_id: UUID,
#     user_data: User,
#     _=Depends(get_current_user_email),
# ):
#     try:
#         user = await db_session.get(User, user_id)
#         if not user:
#             raise HTTPException(
#                 status_code=status.HTTP_404_NOT_FOUND, detail="User not found"
#             )

#         for key, value in user_data.model_dump().items():
#             setattr(user, key, value)

#         await db_session.commit()
#         await db_session.refresh(user)
#         return {"message": "User updated successfully"}
#     except ValueError as e:
#         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))


# @router.delete(
#     "/{user_id}",
#     status_code=status.HTTP_204_NO_CONTENT,
#     responses={
#         status.HTTP_400_BAD_REQUEST: {"description": "Bad Request"},
#         status.HTTP_404_NOT_FOUND: {"description": "Not Found"},
#     },
# )
# async def delete_user(db_session: SessionDep, user_id: UUID):
#     try:
#         user = await db_session.get(User, user_id)
#         if not user:
#             raise HTTPException(
#                 status_code=status.HTTP_404_NOT_FOUND, detail="User not found"
#             )
#         await db_session.delete(user)
#         await db_session.commit()
#     except ValueError as e:
#         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))

================
File: app/config.py
================
from functools import lru_cache

from pydantic import EmailStr
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    openapi_url: str = ""
    environment: str = "development"

    database_url: str = ""
    database_url_async: str = ""

    redis_url: str = ""

    jwt_secret_key: str = ""
    jwt_algorithm: str = "RS256"

    web_app_url: str = "http://localhost:3000"

    resend_api_key: str = ""
    sender_email: EmailStr = ""

    verification_email_expiry_minutes: int = 30
    session_expiry_days: int = 7
    cookie_domain: str = "localhost"

    model_config = SettingsConfigDict(
        env_file=(".env"),
        env_file_encoding="utf-8",
    )


# settings = Settings()


@lru_cache
def get_settings():
    return Settings()

================
File: app/database.py
================
from typing import Annotated

import redis
from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlmodel import SQLModel

from app.config import get_settings


# PostgreSQL (asynchronous)
async_engine = create_async_engine(
    url=get_settings().database_url_async,
    echo=True,
    # Disable the PostgreSQL JIT to improve ENUM datatype handling
    # https://docs.sqlalchemy.org/en/20/dialects/postgresql.html#disabling-the-postgresql-jit-to-improve-enum-datatype-handling
    connect_args={"server_settings": {"jit": "off"}},
)

async_session = async_sessionmaker(
    async_engine,
    class_=AsyncSession,
    expire_on_commit=False,
)


async def drop_tables():
    async with async_engine.begin() as connection:
        await connection.run_sync(SQLModel.metadata.drop_all)


async def create_tables():
    async with async_engine.begin() as connection:
        await connection.run_sync(SQLModel.metadata.create_all)


async def get_session():
    async with async_session() as session:
        yield session


SessionDep = Annotated[AsyncSession, Depends(get_session)]


# Redis
r = redis.from_url(url=get_settings().redis_url)

================
File: app/dependencies.py
================
import json
from datetime import datetime, timezone

from fastapi import BackgroundTasks, Header, HTTPException, Request, status

from app.config import get_settings
from app.database import r as redis


async def get_token_header(x_token: str = Header()):
    if x_token != "fake-super-secret-token":
        raise HTTPException(status_code=400, detail="X-Token header invalid")


async def get_query_token(token: str):
    if token != "jessica":
        raise HTTPException(status_code=400, detail="No Jessica token provided")


async def get_current_user(
    request: Request, background_tasks: BackgroundTasks
) -> str:
    try:
        session_id = request.cookies.get("session_id")
        if not session_id:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED, detail="Unauthorized."
            )

        session_key = f"session:{session_id}"
        session_value: str = redis.get(session_key)  # type: ignore

        if not session_value:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED, detail="Unauthorized."
            )

        session_json = json.loads(session_value)

        session_json["last_active"] = datetime.now(timezone.utc).isoformat()

        pipe = redis.pipeline()
        pipe.set(session_key, json.dumps(session_json))
        pipe.expire(session_key, get_settings().session_expiry_days * 24 * 60 * 60)
        background_tasks.add_task(pipe.execute)

        return session_json.get("user_email")
    except HTTPException as e:
        raise e
    # except Exception as e:
    #     raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))

================
File: app/main.py
================
import time
from contextlib import asynccontextmanager
from datetime import datetime

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import ORJSONResponse

from app.api.v1.internal import admin
from app.api.v1.routers import auth, users
from app.config import get_settings


@asynccontextmanager
async def lifespan(app: FastAPI):
    if get_settings().environment == "production":
        from app.database import create_tables, drop_tables
        await drop_tables()
        await create_tables()
    yield


app = FastAPI(
    title="Connector API",
    openapi_url=get_settings().openapi_url,
    lifespan=lifespan,
    default_response_class=ORJSONResponse,
)


origins = [
    "http://localhost:3000",
    "https://localhost:3000",
    "https://connector.rocks",
    "https://www.connector.rocks",
]


app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


app.include_router(admin.router)
app.include_router(users.router)
app.include_router(auth.router)


@app.get("/", response_model=dict[str, str])
async def root():
    return {
        "message": "Hello Connectors!",
        "current_date": datetime.today().isoformat(),
    }


@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    start_time = time.perf_counter()
    response = await call_next(request)
    process_time = time.perf_counter() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response


# @app.exception_handler(RequestValidationError)
# async def validation_exception_handler(request: Request, exc: RequestValidationError):
#     """
#     Custom exception handler for validation errors
#     """
#     return ORJSONResponse(
#         status_code=422,
#         content={
#             "message": "Validation error",
#             "details": [
#                 {"loc": error["loc"], "message": error["message"], "type": error["type"]}
#                 for error in exc.errors()
#             ],
#         },
#     )

================
File: app/models.py
================
import enum
from datetime import datetime, timezone
from typing import Annotated, Optional
from uuid import UUID, uuid4

from pydantic import AfterValidator, EmailStr
from sqlmodel import (
    ARRAY,
    Column,
    DateTime,
    Enum,
    Field,
    Relationship,
    SQLModel,
    String,
)

from app.validators import email_validator


class UserStatus(str, enum.Enum):
    active = "active"
    deactivated = "deactivated"
    deleted = "deleted"


class UserGender(str, enum.Enum):
    male = "male"
    female = "female"
    prefer_not_to_say = "prefer_not_to_say"


class User(SQLModel, table=True):
    __tablename__: str = "users"  # type: ignore

    id: UUID = Field(default_factory=uuid4, primary_key=True)
    created_at: datetime = Field(
        default=datetime.now(timezone.utc), sa_column=Column(DateTime(timezone=True))
    )
    status: UserStatus = Field(
        default=UserStatus.active,
        sa_column=Column(Enum(UserStatus, name="user_status")),
    )

    email: Annotated[EmailStr, AfterValidator(email_validator)] = Field(
        sa_column=Column(String, index=True, unique=True)
    )
    username: Optional[str] = Field(default=None, index=True, unique=True, min_length=3)
    name: Optional[str] = Field(default=None, min_length=3, max_length=30)
    gender: Optional[UserGender] = Field(
        default=None, sa_column=Column(Enum(UserGender, name="user_gender"))
    )
    profile_picture: Optional[str] = Field(default=None)
    bio: Optional[str] = Field(default=None)
    is_private: bool = Field(default=False)

    threads: list["Thread"] = Relationship(back_populates="user", cascade_delete=True)
    replies: list["Reply"] = Relationship(back_populates="user", cascade_delete=True)


class Thread(SQLModel, table=True):
    __tablename__: str = "threads"  # type: ignore

    id: UUID = Field(default_factory=uuid4, primary_key=True)
    created_at: datetime = Field(
        default=datetime.now(timezone.utc), sa_column=Column(DateTime(timezone=True))
    )
    updated_at: datetime = Field(
        default=datetime.now(timezone.utc), sa_column=Column(DateTime(timezone=True))
    )

    content: str = Field(index=True)
    media: list[str] = Field(sa_column=Column(ARRAY(String)))
    likes: int = Field(default=0)
    edited: bool = Field(default=False)

    replies: list["Reply"] = Relationship(back_populates="thread", cascade_delete=True)

    user_id: Optional[UUID] = Field(default=None, foreign_key="users.id")
    user: Optional["User"] = Relationship(back_populates="threads")


class Reply(SQLModel, table=True):
    __tablename__: str = "replies"  # type: ignore

    id: UUID = Field(default_factory=uuid4, primary_key=True)
    created_at: datetime = Field(
        default=datetime.today(), sa_column=Column(DateTime(timezone=True))
    )

    content: str = Field(index=True)
    media: list[str] = Field(sa_column=Column(ARRAY(String)))
    likes: int = Field(default=0)

    user_id: Optional[UUID] = Field(default=None, foreign_key="users.id")
    user: Optional["User"] = Relationship(back_populates="replies")

    thread_id: Optional[UUID] = Field(default=None, foreign_key="threads.id")
    thread: Optional["Thread"] = Relationship(back_populates="replies")

================
File: app/validators.py
================
from pydantic import EmailStr, validate_email


def email_validator(email: EmailStr):
    _, normalized_email = validate_email(email.lower())

    BLACKLISTED_DOMAINS = ["tempmail.com", "disposable.com"]
    domain = normalized_email.split("@")[1]
    if domain in BLACKLISTED_DOMAINS:
        raise ValueError("Email domain not allowed")

    return email

================
File: compose.yaml
================
services:
  api:
    build: .
    restart: always
    # ports:
    # - "10001:10000" # HTTP only
    environment:
      - PORT=10000
    depends_on:
      proxy:
        condition: service_started
      postgres:
        condition: service_healthy
      redis-stack:
        condition: service_healthy

  proxy:
    build: ./proxy
    restart: always
    ports:
      - "10000:443" # HTTPS support

  postgres:
    image: postgres:latest
    restart: always
    # ports:
    #   - "5432:5432"
    environment:
      - POSTGRES_USER=default
      - POSTGRES_PASSWORD=postgres_password
      - POSTGRES_DB=platform
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready"]
      interval: 1s
      timeout: 5s
      retries: 10

  redis-stack:
    image: redis/redis-stack-server:latest
    # ports:
    #   - "6379:6379"
    environment:
      - REDIS_ARGS=--requirepass redis_password
    volumes:
      - redis_data:/data
    restart: always
    healthcheck:
      test:
        ["CMD", "redis-cli", "-a", "redis_password", "--raw", "incr", "ping"]

volumes:
  postgres_data:
  redis_data:

================
File: Dockerfile
================
# Use an official Python runtime as a parent image
FROM python:3.11

RUN adduser --disabled-password --gecos '' localuser
USER localuser

WORKDIR /home/localuser/app

COPY --chown=localuser:localuser . /home/localuser/app/

# RUN mkdir certificates
# RUN openssl genrsa -out certificates/key.pem 2048
# RUN openssl req -new -key certificates/key.pem -out certificates/csr.pem -subj "/C=VN/ST=Ho Chi Minh City/L=Ho Chi Minh City/O=Connector Inc./CN=www.connector.rocks/emailAddress=powoftech@gmail.com"
# RUN openssl x509 -req -days 365 -in certificates/csr.pem -signkey certificates/key.pem -out certificates/cert.pem 

SHELL ["/bin/bash", "-c"]

RUN python -m venv .venv
RUN source ./.venv/bin/activate
RUN python -m pip install --user .

EXPOSE $PORT

CMD ["sh", "-c", "python -m uvicorn app.main:app --host 0.0.0.0 --port ${PORT} --workers 2"]

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Connector Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: proxy/Dockerfile
================
FROM nginx:latest

COPY nginx.conf /etc/nginx/conf.d/default.conf

WORKDIR /etc/nginx

RUN mkdir certificates
RUN openssl genrsa -out certificates/key.pem 2048
RUN openssl req -new -key certificates/key.pem -out certificates/csr.pem -subj "/C=VN/ST=Ho Chi Minh City/L=Ho Chi Minh City/O=Connector Inc./CN=www.connector.rocks/emailAddress=powoftech@gmail.com"
RUN openssl x509 -req -days 365 -in certificates/csr.pem -signkey certificates/key.pem -out certificates/cert.pem

================
File: proxy/nginx.conf
================
server {
    listen 80;
    server_name 127.0.0.1;
    return 301 https://$host$request_uri;
}

server {
    listen 443 ssl;
    server_name 127.0.0.1;

    ssl_certificate_key /etc/nginx/certificates/key.pem;
    ssl_certificate /etc/nginx/certificates/cert.pem;

    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384';
    ssl_prefer_server_ciphers off;

    location / {
        proxy_pass http://api:10000; # Forward requests to your FastAPI app
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}

================
File: pyproject.toml
================
[project]
name = "rest-backend"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "asyncpg>=0.30.0",
    "fastapi[standard]>=0.115.8",
    "orjson>=3.10.15",
    "passlib[bcrypt]>=1.7.4",
    "pydantic-settings>=2.7.1",
    "pyjwt[crypto]>=2.10.1",
    "python-dotenv>=1.0.1",
    "redis>=5.2.1",
    "resend>=2.6.0",
    "sqlalchemy[asyncio]>=2.0.38",
    "sqlmodel>=0.0.22",
    "uvicorn[standard]>=0.34.0",
]

[dependency-groups]
dev = [
    "alembic>=1.14.1",
    "ruff>=0.9.6",
]

================
File: README.md
================
# Connector API

## Installation

### Docker

```shell
docker build -t rest-backend . --progress=plain --no-cache
docker run --name rest-backend -e PORT=10000 -p 10000:10000 -d rest-backend
```

### Local (coming soon)

- Development:

    ```shell
    uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload --reload-include .env
    ```

- Production:

    ```shell
    uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 2
    ```



================================================================
End of Codebase
================================================================
